{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 08:39:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/29 08:39:14 WARN DependencyUtils: Local jar /opt/spark/postgresql-42.7.8.jar does not exist, skipping.\n",
      "25/12/29 08:39:15 INFO SparkContext: Running Spark version 3.4.0\n",
      "25/12/29 08:39:15 INFO ResourceUtils: ==============================================================\n",
      "25/12/29 08:39:15 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/12/29 08:39:15 INFO ResourceUtils: ==============================================================\n",
      "25/12/29 08:39:15 INFO SparkContext: Submitted application: pyspark-shell\n",
      "25/12/29 08:39:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/12/29 08:39:15 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/12/29 08:39:15 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/12/29 08:39:15 INFO SecurityManager: Changing view acls to: root\n",
      "25/12/29 08:39:15 INFO SecurityManager: Changing modify acls to: root\n",
      "25/12/29 08:39:15 INFO SecurityManager: Changing view acls groups to: \n",
      "25/12/29 08:39:15 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/12/29 08:39:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "25/12/29 08:39:15 INFO Utils: Successfully started service 'sparkDriver' on port 37583.\n",
      "25/12/29 08:39:15 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/12/29 08:39:15 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/12/29 08:39:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/12/29 08:39:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/12/29 08:39:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/12/29 08:39:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1084de6a-371f-400e-990d-b9cbf8485435\n",
      "25/12/29 08:39:15 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "25/12/29 08:39:15 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/12/29 08:39:16 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/12/29 08:39:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/29 08:39:16 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "25/12/29 08:39:16 ERROR SparkContext: Failed to add /opt/spark/postgresql-42.7.8.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /opt/spark/postgresql-42.7.8.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1968)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2023)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "25/12/29 08:39:16 INFO Executor: Starting executor ID driver on host 5d662929a363\n",
      "25/12/29 08:39:16 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/12/29 08:39:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44453.\n",
      "25/12/29 08:39:16 INFO NettyBlockTransferService: Server created on 5d662929a363:44453\n",
      "25/12/29 08:39:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/12/29 08:39:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 5d662929a363, 44453, None)\n",
      "25/12/29 08:39:16 INFO BlockManagerMasterEndpoint: Registering block manager 5d662929a363:44453 with 434.4 MiB RAM, BlockManagerId(driver, 5d662929a363, 44453, None)\n",
      "25/12/29 08:39:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 5d662929a363, 44453, None)\n",
      "25/12/29 08:39:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 5d662929a363, 44453, None)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/postgresql-42.7.8.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69aeae9f0df5fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 08:39:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/12/29 08:39:24 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.\n",
      "25/12/29 08:39:25 INFO InMemoryFileIndex: It took 87 ms to list leaf files for 1 paths.\n"
     ]
    }
   ],
   "source": [
    "schema_ddl=\"run INT, event DOUBLE, E1 DOUBLE, px1 DOUBLE, py1 double, pz1 double, pt1 DOUBLE, eta1 DOUBLE, phi1 DOUBLE, Q1 INT, E2 DOUBLE ,px2 DOUBLE, py2 DOUBLE, pz2 DOUBLE ,pt2 DOUBLE, eta2 DOUBLE ,phi2 DOUBLE ,Q2 INT, M DOUBLE\"\n",
    "df = spark.read.csv(\"dielectron.csv\", header=True, schema=schema_ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0e242ea-81b4-4cf2-8552-0da73fd727b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 08:39:35 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/12/29 08:39:35 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/29 08:39:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.8 KiB, free 434.2 MiB)\n",
      "25/12/29 08:39:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)\n",
      "25/12/29 08:39:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 5d662929a363:44453 (size: 34.4 KiB, free: 434.4 MiB)\n",
      "25/12/29 08:39:36 INFO SparkContext: Created broadcast 0 from head at /tmp/ipykernel_56/2384563279.py:1\n",
      "25/12/29 08:39:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/29 08:39:36 INFO SparkContext: Starting job: head at /tmp/ipykernel_56/2384563279.py:1\n",
      "25/12/29 08:39:36 INFO DAGScheduler: Got job 0 (head at /tmp/ipykernel_56/2384563279.py:1) with 1 output partitions\n",
      "25/12/29 08:39:36 INFO DAGScheduler: Final stage: ResultStage 0 (head at /tmp/ipykernel_56/2384563279.py:1)\n",
      "25/12/29 08:39:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/29 08:39:36 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/29 08:39:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at head at /tmp/ipykernel_56/2384563279.py:1), which has no missing parents\n",
      "25/12/29 08:39:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.8 KiB, free 434.2 MiB)\n",
      "25/12/29 08:39:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 434.2 MiB)\n",
      "25/12/29 08:39:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 5d662929a363:44453 (size: 6.7 KiB, free: 434.4 MiB)\n",
      "25/12/29 08:39:36 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/29 08:39:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at head at /tmp/ipykernel_56/2384563279.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/29 08:39:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/12/29 08:39:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (5d662929a363, executor driver, partition 0, PROCESS_LOCAL, 7921 bytes) \n",
      "25/12/29 08:39:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/12/29 08:39:37 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/dielectron.csv, range: 0-4194304, partition values: [empty row]\n",
      "25/12/29 08:39:37 INFO CodeGenerator: Code generated in 334.231971 ms0 + 1) / 1]\n",
      "25/12/29 08:39:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1667 bytes result sent to driver\n",
      "25/12/29 08:39:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 772 ms on 5d662929a363 (executor driver) (1/1)\n",
      "25/12/29 08:39:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/12/29 08:39:37 INFO DAGScheduler: ResultStage 0 (head at /tmp/ipykernel_56/2384563279.py:1) finished in 1.020 s\n",
      "25/12/29 08:39:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/29 08:39:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/12/29 08:39:37 INFO DAGScheduler: Job 0 finished: head at /tmp/ipykernel_56/2384563279.py:1, took 1.077999 s\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(run=147115, event=626787667.0, E1=2.55245, px1=0.783675, py1=-2.42902, pz1=-0.026877, pt1=2.55231, eta1=-0.0105303, phi1=-1.25871, Q1=1, E2=14.2767, px2=4.38735, py2=-13.5851, pz2=-0.142737, pt2=14.276, eta2=-0.0099982, phi2=-1.25842, Q2=-1, M=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7eb5f8-2fda-4be5-9771-5746a6aaab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df = df.filter(col('M').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d58146e-b58b-4614-add9-e8cf93cb2443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run',\n",
       " 'event',\n",
       " 'E1',\n",
       " 'px1',\n",
       " 'py1',\n",
       " 'pz1',\n",
       " 'pt1',\n",
       " 'eta1',\n",
       " 'phi1',\n",
       " 'Q1',\n",
       " 'E2',\n",
       " 'px2',\n",
       " 'py2',\n",
       " 'pz2',\n",
       " 'pt2',\n",
       " 'eta2',\n",
       " 'phi2',\n",
       " 'Q2',\n",
       " 'M']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a6c3234-17ba-4ed0-b085-51e4356e5db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- run: integer (nullable = true)\n",
      " |-- event: double (nullable = true)\n",
      " |-- E1: double (nullable = true)\n",
      " |-- px1: double (nullable = true)\n",
      " |-- py1: double (nullable = true)\n",
      " |-- pz1: double (nullable = true)\n",
      " |-- pt1: double (nullable = true)\n",
      " |-- eta1: double (nullable = true)\n",
      " |-- phi1: double (nullable = true)\n",
      " |-- Q1: integer (nullable = true)\n",
      " |-- E2: double (nullable = true)\n",
      " |-- px2: double (nullable = true)\n",
      " |-- py2: double (nullable = true)\n",
      " |-- pz2: double (nullable = true)\n",
      " |-- pt2: double (nullable = true)\n",
      " |-- eta2: double (nullable = true)\n",
      " |-- phi2: double (nullable = true)\n",
      " |-- Q2: integer (nullable = true)\n",
      " |-- M: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c16e039-c375-4e09-b8c4-31b417694268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/29 08:49:19 INFO FileSourceStrategy: Pushed Filters: IsNotNull(M)\n",
      "25/12/29 08:49:19 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(M#18)\n",
      "25/12/29 08:49:19 INFO CodeGenerator: Code generated in 48.314266 ms\n",
      "25/12/29 08:49:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.8 KiB, free 434.0 MiB)\n",
      "25/12/29 08:49:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)\n",
      "25/12/29 08:49:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 5d662929a363:44453 (size: 34.4 KiB, free: 434.3 MiB)\n",
      "25/12/29 08:49:19 INFO SparkContext: Created broadcast 2 from jdbc at <unknown>:0\n",
      "25/12/29 08:49:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/12/29 08:49:19 INFO SparkContext: Starting job: jdbc at <unknown>:0\n",
      "25/12/29 08:49:19 INFO DAGScheduler: Got job 1 (jdbc at <unknown>:0) with 4 output partitions\n",
      "25/12/29 08:49:19 INFO DAGScheduler: Final stage: ResultStage 1 (jdbc at <unknown>:0)\n",
      "25/12/29 08:49:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/12/29 08:49:19 INFO DAGScheduler: Missing parents: List()\n",
      "25/12/29 08:49:19 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at jdbc at <unknown>:0), which has no missing parents\n",
      "25/12/29 08:49:19 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 41.4 KiB, free 433.9 MiB)\n",
      "25/12/29 08:49:19 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 433.9 MiB)\n",
      "25/12/29 08:49:19 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 5d662929a363:44453 (size: 17.3 KiB, free: 434.3 MiB)\n",
      "25/12/29 08:49:19 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535\n",
      "25/12/29 08:49:19 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at jdbc at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "25/12/29 08:49:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0\n",
      "25/12/29 08:49:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (5d662929a363, executor driver, partition 0, PROCESS_LOCAL, 7921 bytes) \n",
      "25/12/29 08:49:19 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 5d662929a363:44453 in memory (size: 6.7 KiB, free: 434.3 MiB)\n",
      "25/12/29 08:49:19 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (5d662929a363, executor driver, partition 1, PROCESS_LOCAL, 7921 bytes) \n",
      "25/12/29 08:49:19 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (5d662929a363, executor driver, partition 2, PROCESS_LOCAL, 7921 bytes) \n",
      "25/12/29 08:49:19 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (5d662929a363, executor driver, partition 3, PROCESS_LOCAL, 7921 bytes) \n",
      "25/12/29 08:49:19 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "25/12/29 08:49:19 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
      "25/12/29 08:49:19 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)\n",
      "25/12/29 08:49:19 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)\n",
      "25/12/29 08:49:20 INFO CodeGenerator: Code generated in 76.009899 ms\n",
      "25/12/29 08:49:20 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/dielectron.csv, range: 0-4194304, partition values: [empty row]\n",
      "25/12/29 08:49:20 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/dielectron.csv, range: 4194304-8388608, partition values: [empty row]\n",
      "25/12/29 08:49:20 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/dielectron.csv, range: 12582912-14744032, partition values: [empty row]\n",
      "25/12/29 08:49:20 INFO FileScanRDD: Reading File path: file:///opt/spark/work-dir/dielectron.csv, range: 8388608-12582912, partition values: [empty row]\n",
      "25/12/29 08:49:20 INFO CodeGenerator: Code generated in 11.536635 ms\n",
      "25/12/29 08:49:21 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 5d662929a363:44453 in memory (size: 34.4 KiB, free: 434.3 MiB)\n",
      "25/12/29 08:49:22 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 1691 bytes result sent to driver\n",
      "25/12/29 08:49:22 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 2553 ms on 5d662929a363 (executor driver) (1/4)\n",
      "25/12/29 08:49:23 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 1648 bytes result sent to driver\n",
      "25/12/29 08:49:23 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 3591 ms on 5d662929a363 (executor driver) (2/4)\n",
      "25/12/29 08:49:23 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1648 bytes result sent to driver\n",
      "25/12/29 08:49:23 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 3601 ms on 5d662929a363 (executor driver) (3/4)\n",
      "25/12/29 08:49:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1648 bytes result sent to driver\n",
      "25/12/29 08:49:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 3635 ms on 5d662929a363 (executor driver) (4/4)\n",
      "25/12/29 08:49:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "25/12/29 08:49:23 INFO DAGScheduler: ResultStage 1 (jdbc at <unknown>:0) finished in 3.734 s\n",
      "25/12/29 08:49:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/12/29 08:49:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "25/12/29 08:49:23 INFO DAGScheduler: Job 1 finished: jdbc at <unknown>:0, took 3.746239 s\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.write.jdbc(\n",
    "    url=\"jdbc:postgresql://host.docker.internal:5432/postgres\",\n",
    "    table=\"raw_data\",\n",
    "    mode=\"overwrite\",\n",
    "    properties={\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"password\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259872f-36b6-44c9-a958-9ee38d4ea70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19692cff-1710-4a79-b1fa-311abf8984a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
